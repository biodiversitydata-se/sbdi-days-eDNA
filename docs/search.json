[
  {
    "objectID": "tutorial-01-setup.html",
    "href": "tutorial-01-setup.html",
    "title": "1. Set up the environment",
    "section": "",
    "text": "This tutorial is intended for a pre-configured cloud environment, with a local setup as a fallback."
  },
  {
    "objectID": "tutorial-01-setup.html#goal",
    "href": "tutorial-01-setup.html#goal",
    "title": "1. Get started",
    "section": "Goal",
    "text": "Goal\nAccess the workshop RStudio environment and verify that you can run R code."
  },
  {
    "objectID": "tutorial-01-setup.html#workshop-environment",
    "href": "tutorial-01-setup.html#workshop-environment",
    "title": "1. Get started",
    "section": "Workshop environment",
    "text": "Workshop environment\nFor this workshop, you will work in a pre-configured RStudio environment accessed via a web browser. No local installation is required."
  },
  {
    "objectID": "tutorial-01-setup.html#steps",
    "href": "tutorial-01-setup.html#steps",
    "title": "1. Get started",
    "section": "Steps",
    "text": "Steps\n\n1. Log in\n\nOpen the workshop login link\nSign in using your assigned username and password.\n\n\n\n2. Check that R is running\nIn the Console tab in RStudio, run:\nsessionInfo()"
  },
  {
    "objectID": "tutorial-02-prep.html",
    "href": "tutorial-02-prep.html",
    "title": "2. Prepare data",
    "section": "",
    "text": "In this section we load ASV portal data into R, merge datasets, and prepare count tables for downstream analyses."
  },
  {
    "objectID": "tutorial-02-prep.html#goal",
    "href": "tutorial-02-prep.html#goal",
    "title": "2. Prepare data",
    "section": "Goal",
    "text": "Goal\nLoad, merge and aggregate ASV portal data into a format suitable for ecological analyses."
  },
  {
    "objectID": "tutorial-02-prep.html#steps",
    "href": "tutorial-02-prep.html#steps",
    "title": "2. Prepare data",
    "section": "Steps",
    "text": "Steps\n\nLoad the datasets\nFirst, specify the path to the downloaded ASV portal data:\ndata_path &lt;- \"~/bas_sbdi/sbdi/asvportal_data_downloads/20260202_16S_KTH_BalticSea\"\nThen load the data:\nloaded &lt;- load_data(data_path)\nThis loads all datasets located in the specified folder into the loaded object.\nThe loaded object is a list containing several tables for each dataset, including:\n\ncounts: ASV read counts (stored as sparse matrices)\nevents: sample-level contextual data\nasvs: ASV sequences and taxonomic annotations\nemof: extended measurement or fact data (environmental parameters)\n\nAt this stage, each dataset is kept separate.\n\n\n\nMerge the datasets\nNext, we merge the datasets in order to analyse them jointly:\nmerged &lt;- merge_data(loaded)\nThe merged object has the same overall structure as loaded, but now contains a single combined version of each table (counts, events, asvs, emof).\nTo reduce memory usage, ASV count tables in both loaded$counts and merged$counts are stored as sparse matrices.\n\n\n\nConvert merged data to data frames (optional)\nFor some analyses, it is convenient to work with regular data frames instead of sparse matrices. We can convert the merged object as follows:\nmerged_df &lt;- convert_to_df(merged, convert_counts = TRUE, max_cells = 1e9)\nIn merged_df$counts, each row corresponds to an ASV and each column to a sample. To inspect the sample names:\ncolnames(merged_df$counts)\nWhen converting, ASVs shared across datasets are merged using SBDI ASV IDs. These IDs correspond to MD5 hashes of the sequences, meaning that ASVs with identical sequences are represented only once.\n\n\n\nAggregate counts by taxonomy\nWe now aggregate ASV read counts at different taxonomic levels based on the taxonomic annotation:\ncladecounts &lt;- sum_by_clade(merged$counts, merged$asvs)\nThis produces aggregated counts at the following levels:\n\nkingdom (or domain)\nphylum\nclass\norder\nfamily\ngenus\nspecies\notu (if available)\n\nWe convert the result to data frame format:\ncladecounts_df &lt;- convert_to_df(cladecounts)\nBoth raw counts and normalised counts (relative abundances) are available. For example:\ncladecounts_df$raw$phylum     # raw counts at phylum level\ncladecounts_df$norm$species  # relative abundances at species level\nAt this point, the data are ready for exploration and analysis.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Import and analyse ASV portal data with R",
    "section": "",
    "text": "This tutorial walks through the main steps needed to integrate ASV portal data in R and explore biodiversity patterns. As an example, we analyse microbial diversity along the Baltic Sea salinity gradient.\n\nSet up the environment — Access the workshop RStudio environment (or run locally)\nPrepare data — Load, merge and aggregate data sourced from the ASV portal\nMap samples — Explore sampling coverage across season and salinity\nCommunity structure — Bray–Curtis distances and PCoA across samples\nTaxonomic composition — Taxonomic barplots across salinity basins"
  },
  {
    "objectID": "tutorial.html#integration-of-asv-portal-data-with-asvoccur",
    "href": "tutorial.html#integration-of-asv-portal-data-with-asvoccur",
    "title": "Import and analyse ASV portal data with R",
    "section": "Integration of ASV portal data with asvoccur",
    "text": "Integration of ASV portal data with asvoccur\n\nGet started — Access the workshop RStudio environment in PDC Cloud\nPrepare data — Unpack, merge and aggregate example data sourced from the ASV portal"
  },
  {
    "objectID": "tutorial.html#exploration-of-biodiversity-patterns-across-environments",
    "href": "tutorial.html#exploration-of-biodiversity-patterns-across-environments",
    "title": "Import and analyse ASV portal data with R",
    "section": "Exploration of biodiversity patterns across environments",
    "text": "Exploration of biodiversity patterns across environments\n\nPrepare data — Load, merge and aggregate data sourced from the ASV portal\nMap samples — Explore sampling coverage across season and salinity\nCommunity structure — Bray–Curtis distances and PCoA across samples\nTaxonomic composition — Taxonomic barplots across salinity basins"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring eDNA in SBDI",
    "section": "",
    "text": "Welcome to the Exploring eDNA in SBDI workshop at SBDI Days 2026!"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Exploring eDNA in SBDI",
    "section": "Description",
    "text": "Description\nThis workshop will demonstrate how environmental DNA (eDNA) / metabarcoding data can be accessed, processed, and analyzed within the Swedish Biodiversity Data Infrastructure (SBDI). We will show how to download eDNA data, use our R package to unpack, merge, and aggregate datasets, and then explore biodiversity patterns across environments in R. The session will also introduce SBDI resources for data submission, processing, and integration with GBIF."
  },
  {
    "objectID": "index.html#expected-outcomes",
    "href": "index.html#expected-outcomes",
    "title": "Exploring eDNA in SBDI",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nParticipants will learn about tools and workflows for eDNA data management and analysis, and how these contribute to open biodiversity knowledge."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Exploring eDNA in SBDI",
    "section": "Instructors",
    "text": "Instructors\n\nProf. Anders Andersson — SBDI & KTH SciLifeLab\nDr. Daniel Lundin — SBDI & Linnaeus University\nDr. Maria Prager — SBDI & University of Gothenburg"
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "10 February 2026, 16:00–17:30 William-Olsson lecture hall, Geoscience building, Stockholm University\n\nIntroduction (10 min)\n\nOverview of SBDI and GBIF\neDNA tools and workflows in SBDI\n\nFrom raw reads to ASVs (10 min)\n\nDenoising and taxonomic annotation with nf-core/ampliseq\n\nData sharing and access in SBDI (10 min)\n\nMetabarcoding data submission to ENA\nDiscovery, download, and submission in the ASV portal\nProcessing ASV portal downloads with asvoccur\n\nHands-on tutorial: Import and analyse ASV portal data with R (55 min)\n\nIntegrate ASV portal data with asvoccur\nExplore biodiversity patterns across environments\n\nWrap-up (5 min)\n\nKey takeaways\nSupport and next steps"
  },
  {
    "objectID": "tutorial.html#getting-started",
    "href": "tutorial.html#getting-started",
    "title": "Import and analyse ASV portal data with R",
    "section": "Getting started",
    "text": "Getting started\n\nSet up the environment — Access the workshop RStudio environment (or run locally)"
  },
  {
    "objectID": "tutorial-01-setup.html#option-a-recommended-workshop-rstudio-environment",
    "href": "tutorial-01-setup.html#option-a-recommended-workshop-rstudio-environment",
    "title": "1. Set up the environment",
    "section": "Option A (recommended): Workshop RStudio environment",
    "text": "Option A (recommended): Workshop RStudio environment\nA virtual machine running RStudio Server is available in PDC Cloud. Log in using the credentials provided during the workshop:\nRStudio Server\nFollow the tutorial steps and avoid creating additional large objects, as resources are shared. Save your work regularly as an R script. The virtual machine will be shut down after the workshop, so download any code or output you want to keep via the Files pane (More → Export).\n\nCheck required packages\nLoad asvoccur and confirm version:\nlibrary(asvoccur)\npackageVersion(\"asvoccur\")  # Should say 1.1.1\n\n\nCheck access to example data\nExample ASV portal data is available in a shared directory on the virtual machine. Check access:\ndata_path &lt;- \"/srv/course-data/16S\"\nlist.files(data_path)\nIf two (16S) zip files are listed, you are ready to proceed."
  },
  {
    "objectID": "tutorial-01-setup.html#option-b-fallback-run-rstudio-locally-on-your-laptop",
    "href": "tutorial-01-setup.html#option-b-fallback-run-rstudio-locally-on-your-laptop",
    "title": "1. Set up the environment",
    "section": "Option B (fallback): Run RStudio locally on your laptop",
    "text": "Option B (fallback): Run RStudio locally on your laptop\nOur cloud environment is being tested for the first time. If needed, you can run the tutorial locally.\nYou will need to install:\n\nR\nRStudio\nthe remotes and asvoccur R packages\n\nTo install the required R packages:\ninstall.packages('remotes')\nremotes::install_github(\"biodiversitydata-se/asvoccur\")\nlibrary(asvoccur)\npackageVersion(\"asvoccur\")  # Should say 1.1.1\nIn this option, example data is accessed via a shared Google Drive folder.\nOverview · Next →"
  },
  {
    "objectID": "tutorial-03-map.html",
    "href": "tutorial-03-map.html",
    "title": "3. Map samples",
    "section": "",
    "text": "In this section we explore the spatial and temporal distribution of samples and inspect contextual environmental data."
  },
  {
    "objectID": "tutorial-03-map.html#goal",
    "href": "tutorial-03-map.html#goal",
    "title": "3. Map samples",
    "section": "Goal",
    "text": "Goal\nVisualise sampling coverage across geography, season, and salinity."
  },
  {
    "objectID": "tutorial-03-map.html#steps",
    "href": "tutorial-03-map.html#steps",
    "title": "3. Map samples",
    "section": "Steps",
    "text": "Steps\n\nInspect contextual data\nSample-level contextual information is available in merged_df$events:\ncolnames(merged_df$events)\nSamples are ordered consistently across all tables, which makes it straightforward to link sequence data with contextual data.\nTo simplify downstream analyses, we identify samples belonging to each dataset:\nDS_2013 &lt;- grep(\"KTH-2013-Baltic\", merged_df$events$eventID)\nDS_2019_2020 &lt;- grep(\"PRJEB55296-\", merged_df$events$eventID)\n\n\n\nCheck primers\nWe check which primers were used in the datasets:\nunique(merged_df$events$pcr_primer_name_forward[DS_2013])\nunique(merged_df$events$pcr_primer_name_forward[DS_2019_2020])\n\nunique(merged_df$events$pcr_primer_name_reverse[DS_2013])\nunique(merged_df$events$pcr_primer_name_reverse[DS_2019_2020])\n\n\n\nExtract spatial and temporal variables\nWe extract latitude, longitude, month, and day of year for each sample:\nlat &lt;- merged_df$events$decimalLatitude\nlon &lt;- merged_df$events$decimalLongitude\nmonth &lt;- month(merged_df$events$eventDate)\nyday &lt;- yday(merged_df$events$eventDate)\n\n\n\nInspect EMoF data\nMore detailed environmental parameters are available in merged_df$emof:\ncolnames(merged_df$emof)\nWe now extract salinity and temperature values for all samples.\n\nSalinity\nsalinity &lt;- rep(NA, nrow(merged_df$emof))\nsalinity[DS_2013] &lt;- as.numeric(merged_df$emof$`salinity (psu)`[DS_2013])\nsalinity[DS_2019_2020] &lt;- as.numeric(merged_df$emof$`salinity_average (psu)`[DS_2019_2020])\n\n\nTemperature\ntemperature &lt;- rep(NA, nrow(merged_df$emof))\ntemperature[DS_2013] &lt;- as.numeric(merged_df$emof$`temperature (°C)`[DS_2013])\ntemperature[DS_2019_2020] &lt;- rowMeans(cbind(\n  as.numeric(merged_df$emof$`temperature_water_CTD_0-1m (°C)`[DS_2019_2020]),\n  as.numeric(merged_df$emof$`temperature_water_CTD_5m (°C)`[DS_2019_2020]),\n  as.numeric(merged_df$emof$`temperature_water_CTD_10m (°C)`[DS_2019_2020])\n))\n\n\n\n\nPrepare plotting symbols\nWe define dataset-specific plotting symbols:\npch &lt;- rep(NA, nrow(merged_df$counts))\npch[DS_2013] &lt;- 21\npch[DS_2019_2020] &lt;- 22\nWe also define a colour scale for seasonality:\ncolor_yday &lt;- colorRampPalette(\n  c(\"#2c7fb8\", \"#addd8e\", \"#edf8b1\", \"#fa9fb5\", \"#2c7fb8\")\n)(366)\n\n\n\nMap samples\nWe define a function for plotting samples by month:\nplot_map &lt;- function(dataset) {\n  par(mfrow = c(4, 4), mar = c(3, 3, 3, 3), xpd = TRUE)\n  for (i in 1:12) {\n    ix &lt;- intersect(dataset, which(month == i))\n    newmap &lt;- getMap(resolution = \"low\")\n    plot(\n      newmap,\n      xlim = c(11, 22),\n      ylim = c(62, 63),\n      asp = 1,\n      main = paste(\"Month\", i)\n    )\n    points(\n      lon[ix],\n      lat[ix],\n      col = \"black\",\n      bg = color_yday[yday][ix],\n      pch = pch[ix],\n      cex = 1.5 + as.numeric(salinity[ix]) / 20\n    )\n  }\n\n  plot(\n    1:365, rep(1, 365),\n    col = color_yday,\n    pch = \"|\",\n    cex = 3,\n    axes = FALSE,\n    ylim = c(0.9, 1.3)\n  )\n  axis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\n  text(182, 1.1, \"Day of year\", cex = 1.4)\n\n  plot(\n    c(0, 10, 20), rep(1, 3),\n    col = \"black\",\n    pch = 1,\n    cex = c(1.6, 2.4, 3.2),\n    xlim = c(-5, 25),\n    ylim = c(0.9, 1.3),\n    axes = FALSE\n  )\n  axis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\n  text(2, 1.1, \"Salinity\", cex = 1.4, adj = 0)\n}\nWe apply the function to each dataset:\nplot_map(DS_2013)\nplot_map(DS_2019_2020)\n\n← Previous · Home · Next →"
  }
]