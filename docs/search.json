[
  {
    "objectID": "tutorial-01-setup.html",
    "href": "tutorial-01-setup.html",
    "title": "1. Set up the environment",
    "section": "",
    "text": "Set up the R environment used in the tutorial, either via the workshop cloud environment or locally on your laptop."
  },
  {
    "objectID": "tutorial-01-setup.html#goal",
    "href": "tutorial-01-setup.html#goal",
    "title": "1. Get started",
    "section": "Goal",
    "text": "Goal\nAccess the workshop RStudio environment and verify that you can run R code."
  },
  {
    "objectID": "tutorial-01-setup.html#workshop-environment",
    "href": "tutorial-01-setup.html#workshop-environment",
    "title": "1. Get started",
    "section": "Workshop environment",
    "text": "Workshop environment\nFor this workshop, you will work in a pre-configured RStudio environment accessed via a web browser. No local installation is required."
  },
  {
    "objectID": "tutorial-01-setup.html#steps",
    "href": "tutorial-01-setup.html#steps",
    "title": "1. Get started",
    "section": "Steps",
    "text": "Steps\n\n1. Log in\n\nOpen the workshop login link\nSign in using your assigned username and password.\n\n\n\n2. Check that R is running\nIn the Console tab in RStudio, run:\nsessionInfo()"
  },
  {
    "objectID": "tutorial-02-prep.html",
    "href": "tutorial-02-prep.html",
    "title": "2. Prepare data",
    "section": "",
    "text": "Load, merge and aggregate ASV portal data into a format suitable for downstream ecological analyses.\n\nLoad the datasets\nLoad the two 16S datasets using the data_path object defined in the previous section:\nloaded &lt;- load_data(data_path)\nThis reads datasets into the loaded object, which you can inspect using the magnifying glass next to it in the Environment pane. The object is a list organised into data type sub-lists, each containing one table or matrix element per dataset:\n\ncounts: ASV read counts\nasvs: ASV sequences and taxonomic annotations\nevents: core sample-level metadata (where, when, how, who)\ndatasets: dataset-level metadata\nemof: dataset-specific extended measurement or fact contextual data\n\nSparse matrices (dgCMatrix) are used to reduce memory usage for count data, while tabular structures (data.table) are used for metadata. To see dataset sizes, inspect the elements in counts, where sizes are shown as matrix dimensions (e.g. KTH-2013-Baltic-16S: dgCMatrix [3074 x 21], corresponding to the number of ASVs × samples).\nTo see the full function documentation for load_data:\n?asvoccur::load_data\n\n\n\nMerge the datasets\nNext, we merge the datasets in order to analyse them jointly:\nmerged &lt;- merge_data(loaded)\nThe merged object is a list with one combined object per data type (counts, events, asvs, datasets, emof), represented as a sparse matrix for count data and data tables for metadata, as in loaded.\n\n\n\nConvert merged data to data frames\nFor some analyses, it is convenient to work with regular data frames instead of sparse matrices. Note that converting large count matrices to data frames can be very memory-intensive, but should be feasible for the datasets used here:\nmerged_df &lt;- convert_to_df(merged, convert_counts = TRUE, max_cells = 1e9)\nIn merged_df$counts, each row corresponds to an ASV and each column to a sample. To inspect the sample names:\ncolnames(merged_df$counts)\nWhen converting, ASVs shared across datasets are merged using SBDI ASV IDs. These IDs correspond to MD5 hashes of the sequences, meaning that ASVs with identical sequences are represented only once.\n\n\n\nAggregate counts by taxonomy\nWe now aggregate ASV read counts at different taxonomic levels based on the taxonomic annotation:\ncladecounts &lt;- sum_by_clade(merged$counts, merged$asvs)\nThis produces aggregated counts at the following levels:\n\nkingdom (or domain)\nphylum\nclass\norder\nfamily\ngenus\nspecies\notu (if available)\n\nWe convert the result to data frame format:\ncladecounts_df &lt;- convert_to_df(cladecounts)\nBoth raw counts and normalised counts (relative abundances per sample) are available. For example:\ncladecounts_df$raw$phylum[1:3, 1:3]  # raw counts at phylum level\ncladecounts_df$norm$class[1:3, 1:3]  # relative abundances at class level\nAt this point, the data are ready for exploration and analysis.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-02-prep.html#goal",
    "href": "tutorial-02-prep.html#goal",
    "title": "2. Prepare data",
    "section": "Goal",
    "text": "Goal\nLoad, merge and aggregate ASV portal data into a format suitable for ecological analyses."
  },
  {
    "objectID": "tutorial-02-prep.html#steps",
    "href": "tutorial-02-prep.html#steps",
    "title": "2. Prepare data",
    "section": "Steps",
    "text": "Steps\n\nLoad the datasets\nFirst, specify the path to the downloaded ASV portal data:\ndata_path &lt;- \"~/bas_sbdi/sbdi/asvportal_data_downloads/20260202_16S_KTH_BalticSea\"\nThen load the data:\nloaded &lt;- load_data(data_path)\nThis loads all datasets located in the specified folder into the loaded object.\nThe loaded object is a list containing several tables for each dataset, including:\n\ncounts: ASV read counts (stored as sparse matrices)\nevents: sample-level contextual data\nasvs: ASV sequences and taxonomic annotations\nemof: extended measurement or fact data (environmental parameters)\n\nAt this stage, each dataset is kept separate.\n\n\n\nMerge the datasets\nNext, we merge the datasets in order to analyse them jointly:\nmerged &lt;- merge_data(loaded)\nThe merged object has the same overall structure as loaded, but now contains a single combined version of each table (counts, events, asvs, emof).\nTo reduce memory usage, ASV count tables in both loaded$counts and merged$counts are stored as sparse matrices.\n\n\n\nConvert merged data to data frames (optional)\nFor some analyses, it is convenient to work with regular data frames instead of sparse matrices. We can convert the merged object as follows:\nmerged_df &lt;- convert_to_df(merged, convert_counts = TRUE, max_cells = 1e9)\nIn merged_df$counts, each row corresponds to an ASV and each column to a sample. To inspect the sample names:\ncolnames(merged_df$counts)\nWhen converting, ASVs shared across datasets are merged using SBDI ASV IDs. These IDs correspond to MD5 hashes of the sequences, meaning that ASVs with identical sequences are represented only once.\n\n\n\nAggregate counts by taxonomy\nWe now aggregate ASV read counts at different taxonomic levels based on the taxonomic annotation:\ncladecounts &lt;- sum_by_clade(merged$counts, merged$asvs)\nThis produces aggregated counts at the following levels:\n\nkingdom (or domain)\nphylum\nclass\norder\nfamily\ngenus\nspecies\notu (if available)\n\nWe convert the result to data frame format:\ncladecounts_df &lt;- convert_to_df(cladecounts)\nBoth raw counts and normalised counts (relative abundances) are available. For example:\ncladecounts_df$raw$phylum     # raw counts at phylum level\ncladecounts_df$norm$species  # relative abundances at species level\nAt this point, the data are ready for exploration and analysis.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Import and analyse ASV portal data with R",
    "section": "",
    "text": "This tutorial walks through the main steps needed to integrate ASV portal data in R and explore biodiversity patterns. As a case study, we examine microbial diversity along the Baltic Sea salinity gradient using data from two published studies (see references below).\nIn the first study, 21 surface-water samples collected during a research cruise in summer 2013 were analyzed using 16S and 18S rRNA gene metabarcoding. In the second study, 331 surface samples (16S) and 333 surface samples (18S) were sequenced from 22 stations over a 13-month period (2019–2020).\nSections 1–6 focus on the 16S data. Because the same PCR primers were used in both datasets, many ASVs are shared, enabling direct integration of the data even at the ASV level. Section 7 examines the 18S data and demonstrates how datasets can be co-analyzed even when different primers were used and no ASVs overlap.\n\nSet up the environment — Access the workshop RStudio environment (or run locally)\nPrepare data — Load, merge and aggregate data sourced from the ASV portal\nMap samples — Explore sampling coverage across season and salinity\nCommunity structure — Bray–Curtis distances and PCoA across samples\nTaxonomic composition — Taxonomic barplots across salinity basins\nPredict environmental parameters — Random Forest prediction from community composition\nAnalyse 18S data — Genus-level Bray–Curtis and PCoA on 18S data\n\nGet started →\n\nReferences\nHu, Y.O.O., Karlson, B., Charvet, S., Andersson, A.F. (2016). Diversity of pico- to mesoplankton along the 2000 km salinity gradient of the Baltic Sea. Frontiers in Microbiology, 7:679: 16S rRNA, 18S rDNA\nLatz, M.A.C., Andersson, A., Brugel, S., Hedblom, M., Jurdzinski, K., Karlson, B., Lindh, M., Lycken, J., Torstenson, A., Andersson, A.F. (2024). A comprehensive dataset on spatiotemporal variation of microbial plankton communities in the Baltic Sea. Scientific Data, 11:18: 16S rRNA, 18S rDNA"
  },
  {
    "objectID": "tutorial.html#integration-of-asv-portal-data-with-asvoccur",
    "href": "tutorial.html#integration-of-asv-portal-data-with-asvoccur",
    "title": "Import and analyse ASV portal data with R",
    "section": "Integration of ASV portal data with asvoccur",
    "text": "Integration of ASV portal data with asvoccur\n\nGet started — Access the workshop RStudio environment in PDC Cloud\nPrepare data — Unpack, merge and aggregate example data sourced from the ASV portal"
  },
  {
    "objectID": "tutorial.html#exploration-of-biodiversity-patterns-across-environments",
    "href": "tutorial.html#exploration-of-biodiversity-patterns-across-environments",
    "title": "Import and analyse ASV portal data with R",
    "section": "Exploration of biodiversity patterns across environments",
    "text": "Exploration of biodiversity patterns across environments\n\nPrepare data — Load, merge and aggregate data sourced from the ASV portal\nMap samples — Explore sampling coverage across season and salinity\nCommunity structure — Bray–Curtis distances and PCoA across samples\nTaxonomic composition — Taxonomic barplots across salinity basins"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring eDNA in SBDI",
    "section": "",
    "text": "Welcome to the Exploring eDNA in SBDI workshop at SBDI Days 2026!"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Exploring eDNA in SBDI",
    "section": "Description",
    "text": "Description\nThis workshop will demonstrate how environmental DNA (eDNA) / metabarcoding data can be accessed, processed, and analyzed within the Swedish Biodiversity Data Infrastructure (SBDI). We will show how to download eDNA data, use our R package to unpack, merge, and aggregate datasets, and then explore biodiversity patterns across environments in R. The session will also introduce SBDI resources for data submission, processing, and integration with GBIF."
  },
  {
    "objectID": "index.html#expected-outcomes",
    "href": "index.html#expected-outcomes",
    "title": "Exploring eDNA in SBDI",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nParticipants will learn about tools and workflows for eDNA data management and analysis, and how these contribute to open biodiversity knowledge."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Exploring eDNA in SBDI",
    "section": "Instructors",
    "text": "Instructors\n\nProf. Anders Andersson — SBDI & KTH SciLifeLab\nDr. Daniel Lundin — SBDI & Linnaeus University\nDr. Maria Prager — SBDI & University of Gothenburg"
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "10 February 2026, 16:00–17:30 William-Olsson lecture hall, Geoscience building, Stockholm University\n\nIntroduction (10 min)\n\nOverview of SBDI and GBIF\neDNA tools and workflows in SBDI\n\nFrom raw reads to ASVs (10 min)\n\nDenoising and taxonomic annotation with nf-core/ampliseq\n\nData sharing and access in SBDI (10 min)\n\nMetabarcoding data submission to ENA\nSubmission, discovery and download in the ASV portal\nProcessing ASV portal downloads with asvoccur\n\nHands-on tutorial: Import and analyse ASV portal data with R (55 min)\n\nIntegrate ASV portal data with asvoccur\nExplore biodiversity patterns across environments\n\nWrap-up (5 min)\n\nKey takeaways\nSupport and next steps"
  },
  {
    "objectID": "tutorial.html#getting-started",
    "href": "tutorial.html#getting-started",
    "title": "Import and analyse ASV portal data with R",
    "section": "Getting started",
    "text": "Getting started\n\nSet up the environment — Access the workshop RStudio environment (or run locally)"
  },
  {
    "objectID": "tutorial-01-setup.html#option-a-recommended-workshop-rstudio-environment",
    "href": "tutorial-01-setup.html#option-a-recommended-workshop-rstudio-environment",
    "title": "1. Set up the environment",
    "section": "Option A (recommended): Workshop RStudio environment",
    "text": "Option A (recommended): Workshop RStudio environment\nA virtual machine running RStudio Server is available in PDC Cloud. Log in using the credentials provided during the workshop:\nRStudio Server\nFollow the tutorial steps and avoid creating additional large objects, as resources are shared. Save your work regularly as an R script. The virtual machine will be shut down after the workshop, so download any code or output you want to keep via the Files pane (More → Export).\n\nCheck required packages\nAll required packages are pre-installed in the workshop environment. Load them and confirm that they are available:\nlibrary(asvoccur)\nlibrary(ranger)\nlibrary(vegan)\nlibrary(ape)\nlibrary(rworldmap)\nlibrary(data.table)\n\n\nCheck access to example data\nFor this tutorial, ASV portal data is pre-downloaded and available on the virtual machine. Check access:\ndata_path &lt;- \"/srv/course-data/16S\"\nlist.files(data_path)\nIf two (16S) zip files are listed, you are ready to proceed."
  },
  {
    "objectID": "tutorial-01-setup.html#option-b-fallback-run-rstudio-locally-on-your-laptop",
    "href": "tutorial-01-setup.html#option-b-fallback-run-rstudio-locally-on-your-laptop",
    "title": "1. Set up the environment",
    "section": "Option B (fallback): Run RStudio locally on your laptop",
    "text": "Option B (fallback): Run RStudio locally on your laptop\nOur cloud environment is being tested for the first time. If needed, you can run the tutorial locally.\nYou will need to install:\n\nR\nRStudio\nthe following R packages:\n\nremotes\nasvoccur\nranger\nvegan\nape\nrworldmap\ndata.table\n\n\nTo install and load the required R packages:\n# Install from CRAN\ninstall.packages(c(\n  \"remotes\",\n  \"ranger\",\n  \"vegan\",\n  \"ape\",\n  \"rworldmap\",\n  \"data.table\"\n))\n\n# Install from GitHub\nremotes::install_github(\"biodiversitydata-se/asvoccur\")\n\n# Load\nlibrary(asvoccur)\nlibrary(ranger)\nlibrary(vegan)\nlibrary(ape)\nlibrary(rworldmap)\nlibrary(data.table)\n\n# Confirm version\npackageVersion(\"asvoccur\")  # Should say 1.1.1\n\nDownload example data from the ASV portal\nIn this option, example data is downloaded directly from the ASV portal.\n\nGo to the ASV portal start page.\nClick DOWNLOAD. If you are not already logged in, you will be redirected to the login page.\nIf you do not already have an account, either:\n\nsign in with a Google or GitHub account, or\ncreate a new SBDI account.\n\nOn the Download page, under Download link, download the following datasets (note the underlined project IDs to distinguish similar titles):\n\nPRJEB55296-16S\nKTH-2013-Baltic-16S\nPRJEB55296-18S\nKTH-2013-Baltic-18S\n\nAfter downloading, organise the data locally into two sub-folders:\ncourse-data/\n├── 16S/\n│   ├── PRJEB55296-16S.zip\n│   └── KTH-2013-Baltic-16S.zip\n└── 18S/\n    ├── PRJEB55296-18S.zip\n    └── KTH-2013-Baltic-18S.zip\nSet the path to the 16S data folder in R and confirm access:\n\n       data_path &lt;- \"local/path/to/course-data/16S\"\n       list.files(data_path)\nIf “KTH-2013-Baltic-16S.zip” and “PRJEB55296-16S.zip” are listed, you are ready to proceed.\nOverview · Next →"
  },
  {
    "objectID": "tutorial-03-map.html",
    "href": "tutorial-03-map.html",
    "title": "3. Map samples",
    "section": "",
    "text": "Explore the spatial and seasonal distribution of samples and inspect associated environmental data.\n\nInspect contextual data\nCore sample-level metadata is available in merged_df$events:\ncolnames(merged_df$events)\nSamples are ordered consistently across tables, which makes it straightforward to link sequence data with contextual data. Thus, rownames(merged_df$events), rownames(merged_df$emof) and colnames(merged_df$emof) are the same. Likewise, ASVs are ordered consistently between merged_df$counts and merged_df$asvs.\nTo simplify downstream analyses, we identify samples belonging to each dataset:\nDS_2013 &lt;- grep(\"KTH-2013-Baltic\", rownames(merged_df$events))\nDS_2019_2020 &lt;- grep(\"PRJEB55296\", rownames(merged_df$events))\n\n\n\nCheck primers\nWe check which primers were used in the datasets:\nunique(merged_df$events$pcr_primer_name_forward[DS_2013])\nunique(merged_df$events$pcr_primer_name_forward[DS_2019_2020])\n\nunique(merged_df$events$pcr_primer_name_reverse[DS_2013])\nunique(merged_df$events$pcr_primer_name_reverse[DS_2019_2020])\nThis verifies that the same primers were used in the two datasets.\n\n\n\nExtract spatial and temporal variables\nWe extract latitude, longitude, month, and day of year for each sample:\nlat &lt;- merged_df$events$decimalLatitude\nlon &lt;- merged_df$events$decimalLongitude\nmonth &lt;- month(merged_df$events$eventDate)\nyday &lt;- yday(merged_df$events$eventDate)\n\n\n\nInspect EMoF data\nAdditional contextual variables are available in merged_df$emof. Unlike fields in other metadata tables, EMoF variables are dataset-specific, meaning that the same variable may be stored under different column names. These variables therefore need to be identified and harmonised before analysis. We start by inspecting the column names:\ncolnames(merged_df$emof)\n\nSalinity\nWe now extract salinity for all samples. Because these variables are recorded differently in the two datasets, values are taken from different columns.\nsalinity &lt;- rep(NA, nrow(merged_df$emof))\nsalinity[DS_2013] &lt;- as.numeric(merged_df$emof$`salinity (psu)`[DS_2013])\nsalinity[DS_2019_2020] &lt;- as.numeric(merged_df$emof$`salinity_average (psu)`[DS_2019_2020])\n\n\n\n\nPrepare plotting symbols\nWe define dataset-specific plotting symbols:\npch &lt;- rep(NA, nrow(merged_df$counts))\npch[DS_2013] &lt;- 21\npch[DS_2019_2020] &lt;- 22\nWe also define a colour scale for seasonality:\ncolor_yday &lt;- colorRampPalette(\n  c(\"#2c7fb8\", \"#addd8e\", \"#edf8b1\", \"#fa9fb5\", \"#2c7fb8\")\n)(366)\n\n\n\nMap samples\nWe define a function that plots monthly maps of the study area, showing where and when samples were collected, with point colour indicating sampling date and point size indicating salinity.\nplot_map &lt;- function(dataset) {\n  par(mfrow = c(4, 4), mar = c(3, 3, 3, 3), xpd = TRUE)\n  for (i in 1:12) {\n    ix &lt;- intersect(dataset, which(month == i))\n    newmap &lt;- getMap(resolution = \"low\")\n    plot(\n      newmap,\n      xlim = c(11, 22),\n      ylim = c(62, 63),\n      asp = 1,\n      main = paste(\"Month\", i)\n    )\n    points(\n      lon[ix],\n      lat[ix],\n      col = \"black\",\n      bg = color_yday[yday][ix],\n      pch = pch[ix],\n      cex = 1.5 + as.numeric(salinity[ix]) / 20\n    )\n  }\n\n  plot(\n    1:365, rep(1, 365),\n    col = color_yday,\n    pch = \"|\",\n    cex = 3,\n    axes = FALSE,\n    ylim = c(0.9, 1.3)\n  )\n  axis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\n  text(182, 1.1, \"Day of year\", cex = 1.4)\n\n  plot(\n    c(0, 10, 20), rep(1, 3),\n    col = \"black\",\n    pch = 1,\n    cex = c(1.6, 2.4, 3.2),\n    xlim = c(-5, 25),\n    ylim = c(0.9, 1.3),\n    axes = FALSE\n  )\n  axis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\n  text(2, 1.1, \"Salinity\", cex = 1.4, adj = 0)\n}\nWe apply the function to plot the DS_2013 dataset:\nplot_map(DS_2013)\nAnd the DS_2019_2020 dataset:\nplot_map(DS_2019_2020)\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-03-map.html#goal",
    "href": "tutorial-03-map.html#goal",
    "title": "3. Map samples",
    "section": "Goal",
    "text": "Goal\nVisualise sampling coverage across geography, season, and salinity."
  },
  {
    "objectID": "tutorial-03-map.html#steps",
    "href": "tutorial-03-map.html#steps",
    "title": "3. Map samples",
    "section": "Steps",
    "text": "Steps\n\nInspect contextual data\nSample-level contextual information is available in merged_df$events:\ncolnames(merged_df$events)\nSamples are ordered consistently across all tables, which makes it straightforward to link sequence data with contextual data.\nTo simplify downstream analyses, we identify samples belonging to each dataset:\nDS_2013 &lt;- grep(\"KTH-2013-Baltic\", merged_df$events$eventID)\nDS_2019_2020 &lt;- grep(\"PRJEB55296-\", merged_df$events$eventID)\n\n\n\nCheck primers\nWe check which primers were used in the datasets:\nunique(merged_df$events$pcr_primer_name_forward[DS_2013])\nunique(merged_df$events$pcr_primer_name_forward[DS_2019_2020])\n\nunique(merged_df$events$pcr_primer_name_reverse[DS_2013])\nunique(merged_df$events$pcr_primer_name_reverse[DS_2019_2020])\n\n\n\nExtract spatial and temporal variables\nWe extract latitude, longitude, month, and day of year for each sample:\nlat &lt;- merged_df$events$decimalLatitude\nlon &lt;- merged_df$events$decimalLongitude\nmonth &lt;- month(merged_df$events$eventDate)\nyday &lt;- yday(merged_df$events$eventDate)\n\n\n\nInspect EMoF data\nMore detailed environmental parameters are available in merged_df$emof:\ncolnames(merged_df$emof)\nWe now extract salinity and temperature values for all samples.\n\nSalinity\nsalinity &lt;- rep(NA, nrow(merged_df$emof))\nsalinity[DS_2013] &lt;- as.numeric(merged_df$emof$`salinity (psu)`[DS_2013])\nsalinity[DS_2019_2020] &lt;- as.numeric(merged_df$emof$`salinity_average (psu)`[DS_2019_2020])\n\n\nTemperature\ntemperature &lt;- rep(NA, nrow(merged_df$emof))\ntemperature[DS_2013] &lt;- as.numeric(merged_df$emof$`temperature (°C)`[DS_2013])\ntemperature[DS_2019_2020] &lt;- rowMeans(cbind(\n  as.numeric(merged_df$emof$`temperature_water_CTD_0-1m (°C)`[DS_2019_2020]),\n  as.numeric(merged_df$emof$`temperature_water_CTD_5m (°C)`[DS_2019_2020]),\n  as.numeric(merged_df$emof$`temperature_water_CTD_10m (°C)`[DS_2019_2020])\n))\n\n\n\n\nPrepare plotting symbols\nWe define dataset-specific plotting symbols:\npch &lt;- rep(NA, nrow(merged_df$counts))\npch[DS_2013] &lt;- 21\npch[DS_2019_2020] &lt;- 22\nWe also define a colour scale for seasonality:\ncolor_yday &lt;- colorRampPalette(\n  c(\"#2c7fb8\", \"#addd8e\", \"#edf8b1\", \"#fa9fb5\", \"#2c7fb8\")\n)(366)\n\n\n\nMap samples\nWe define a function for plotting samples by month:\nplot_map &lt;- function(dataset) {\n  par(mfrow = c(4, 4), mar = c(3, 3, 3, 3), xpd = TRUE)\n  for (i in 1:12) {\n    ix &lt;- intersect(dataset, which(month == i))\n    newmap &lt;- getMap(resolution = \"low\")\n    plot(\n      newmap,\n      xlim = c(11, 22),\n      ylim = c(62, 63),\n      asp = 1,\n      main = paste(\"Month\", i)\n    )\n    points(\n      lon[ix],\n      lat[ix],\n      col = \"black\",\n      bg = color_yday[yday][ix],\n      pch = pch[ix],\n      cex = 1.5 + as.numeric(salinity[ix]) / 20\n    )\n  }\n\n  plot(\n    1:365, rep(1, 365),\n    col = color_yday,\n    pch = \"|\",\n    cex = 3,\n    axes = FALSE,\n    ylim = c(0.9, 1.3)\n  )\n  axis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\n  text(182, 1.1, \"Day of year\", cex = 1.4)\n\n  plot(\n    c(0, 10, 20), rep(1, 3),\n    col = \"black\",\n    pch = 1,\n    cex = c(1.6, 2.4, 3.2),\n    xlim = c(-5, 25),\n    ylim = c(0.9, 1.3),\n    axes = FALSE\n  )\n  axis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\n  text(2, 1.1, \"Salinity\", cex = 1.4, adj = 0)\n}\nWe apply the function to each dataset:\nplot_map(DS_2013)\nplot_map(DS_2019_2020)\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-04-pcoa.html",
    "href": "tutorial-04-pcoa.html",
    "title": "4. Community structure (PCoA)",
    "section": "",
    "text": "Analyse community composition using Bray–Curtis distances and visualise patterns with a Principal Coordinates Analysis (PCoA).\n\nNormalise counts to relative abundances\nFirst, convert ASV counts to relative abundances per sample:\nnorm_counts &lt;- t(t(merged_df$counts) / colSums(merged_df$counts, na.rm = TRUE))\n\n\nCalculate Bray–Curtis distances\nCompute pairwise Bray–Curtis distances (beta-diversity):\nbray_dist &lt;- as.matrix(vegdist(t(norm_counts), method = \"bray\"))\n\n\nRun PCoA\nPerform Principal Coordinates Analysis on the distance matrix:\npcoa &lt;- pcoa(bray_dist, correction = \"cailliez\")\n\n\nPlot PCoA results\nWe plot samples in PCoA space using the first two axes and show dataset-specific panels and legends.\nlayout(matrix(c(1,1,1,1,2,2,2,2,3,4), 5, 2, byrow = TRUE))\npar(mar = c(5, 5, 1, 1), xpd = TRUE, cex.axis = 1)\n\nxlab &lt;- paste(\"PC1 \", round(pcoa$values$Eigenvalues[1]), \"%\", sep = \"\")\nylab &lt;- paste(\"PC2 \", round(pcoa$values$Eigenvalues[2]), \"%\", sep = \"\")\n\n# Panel: 2019_2020\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2019_2020\")\npoints(pcoa$vectors[DS_2019_2020,1], pcoa$vectors[DS_2019_2020,2],\n       pch = pch[DS_2019_2020], col = \"black\",\n       bg = color_yday[yday[DS_2019_2020]],\n       cex = 1.0 + as.numeric(salinity[DS_2019_2020]) / 10)\n\n# Panel: 2013\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2013\")\npoints(pcoa$vectors[DS_2013,1], pcoa$vectors[DS_2013,2],\n       pch = pch[DS_2013], col = \"black\",\n       bg = color_yday[yday[DS_2013]],\n       cex = 1.0 + as.numeric(salinity[DS_2013]) / 10)\n\n# Legend: day of year colour bar\nplot(1:365, rep(1, 365), col = color_yday, pch = \"|\", cex = 3, axes = FALSE, ylim = c(0.9, 1.3))\naxis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\ntext(182, 1.2, \"Day of year\", cex = 2)\n\n# Legend: salinity size scale\nplot(c(0, 10, 20), rep(1, 3),\n     col = \"black\", pch = 1, cex = c(1.6, 2.4, 3.2),\n     xlim = c(-5, 25), ylim = c(0.9, 1.3), axes = FALSE, ylab = \"\", xlab = \"\")\naxis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\ntext(10, 1.2, \"Salinity\", cex = 2)\nInterpretation: the PCoA shows that community composition is structured by both salinity and season.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-04-pcoa.html#goal",
    "href": "tutorial-04-pcoa.html#goal",
    "title": "4. Community structure (PCoA)",
    "section": "Goal",
    "text": "Goal\nCompute pairwise community distances and visualise sample clustering with PCoA, highlighting season and salinity."
  },
  {
    "objectID": "tutorial-04-pcoa.html#steps",
    "href": "tutorial-04-pcoa.html#steps",
    "title": "4. Community structure (PCoA)",
    "section": "Steps",
    "text": "Steps\n\nNormalise counts to relative abundances\nFirst, convert ASV counts to relative abundances per sample:\nnorm_counts &lt;- t(t(merged_df$counts) / colSums(merged_df$counts, na.rm = TRUE))\n#norm_counts[which(is.na(norm_counts))] &lt;- 0\n\n\nCalculate Bray–Curtis distances\nCompute pairwise Bray–Curtis distances (beta-diversity):\nbray_dist &lt;- as.matrix(vegdist(t(norm_counts), method = \"bray\"))\n\n\nRun PCoA\nPerform Principal Coordinates Analysis on the distance matrix:\npcoa &lt;- pcoa(bray_dist, correction = \"cailliez\")\n\n\nPlot PCoA results\nWe plot samples in PCoA space using the first two axes and show dataset-specific panels and legends.\nlayout(matrix(c(1,1,1,1,2,2,2,2,3,4), 5, 2, byrow = TRUE))\npar(mar = c(5, 5, 1, 1), xpd = TRUE, cex.axis = 1)\n\nxlab &lt;- paste(\"PC1 \", round(pcoa$values$Eigenvalues[1]), \"%\", sep = \"\")\nylab &lt;- paste(\"PC2 \", round(pcoa$values$Eigenvalues[2]), \"%\", sep = \"\")\n\n# Panel: 2019_2020\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2019_2020\")\npoints(pcoa$vectors[DS_2019_2020,1], pcoa$vectors[DS_2019_2020,2],\n       pch = pch[DS_2019_2020], col = \"black\",\n       bg = color_yday[yday[DS_2019_2020]],\n       cex = 1.0 + as.numeric(salinity[DS_2019_2020]) / 10)\n\n# Panel: 2013\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2013\")\npoints(pcoa$vectors[DS_2013,1], pcoa$vectors[DS_2013,2],\n       pch = pch[DS_2013], col = \"black\",\n       bg = color_yday[yday[DS_2013]],\n       cex = 1.0 + as.numeric(salinity[DS_2013]) / 10)\n\n# Legend: day of year colour bar\nplot(1:365, rep(1, 365), col = color_yday, pch = \"|\", cex = 3, axes = FALSE, ylim = c(0.9, 1.3))\naxis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\ntext(182, 1.2, \"Day of year\", cex = 2)\n\n# Legend: salinity size scale\nplot(c(0, 10, 20), rep(1, 3),\n     col = \"black\", pch = 1, cex = c(1.6, 2.4, 3.2),\n     xlim = c(-5, 25), ylim = c(0.9, 1.3), axes = FALSE, ylab = \"\", xlab = \"\")\naxis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\ntext(10, 1.2, \"Salinity\", cex = 2)\nInterpretation: the PCoA shows that community composition is structured by both salinity and season.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-05.html",
    "href": "tutorial-05.html",
    "title": "3. Map samples",
    "section": "",
    "text": "In this section we explore the spatial and temporal distribution of samples and inspect contextual environmental data."
  },
  {
    "objectID": "tutorial-05.html#goal",
    "href": "tutorial-05.html#goal",
    "title": "3. Map samples",
    "section": "Goal",
    "text": "Goal\nVisualise sampling coverage across geography, season, and salinity."
  },
  {
    "objectID": "tutorial-05.html#steps",
    "href": "tutorial-05.html#steps",
    "title": "3. Map samples",
    "section": "Steps",
    "text": "Steps\n\nInspect contextual data\nSample-level contextual information is available in merged_df$events:\ncolnames(merged_df$events)\nSamples are ordered consistently across all tables, which makes it straightforward to link sequence data with contextual data.\nTo simplify downstream analyses, we identify samples belonging to each dataset:\nDS_2013 &lt;- grep(\"KTH-2013-Baltic\", merged_df$events$eventID)\nDS_2019_2020 &lt;- grep(\"PRJEB55296-\", merged_df$events$eventID)\n\n\n\nCheck primers\nWe check which primers were used in the datasets:\nunique(merged_df$events$pcr_primer_name_forward[DS_2013])\nunique(merged_df$events$pcr_primer_name_forward[DS_2019_2020])\n\nunique(merged_df$events$pcr_primer_name_reverse[DS_2013])\nunique(merged_df$events$pcr_primer_name_reverse[DS_2019_2020])\n\n\n\nExtract spatial and temporal variables\nWe extract latitude, longitude, month, and day of year for each sample:\nlat &lt;- merged_df$events$decimalLatitude\nlon &lt;- merged_df$events$decimalLongitude\nmonth &lt;- month(merged_df$events$eventDate)\nyday &lt;- yday(merged_df$events$eventDate)\n\n\n\nInspect EMoF data\nMore detailed environmental parameters are available in merged_df$emof:\ncolnames(merged_df$emof)\nWe now extract salinity and temperature values for all samples.\n\nSalinity\nsalinity &lt;- rep(NA, nrow(merged_df$emof))\nsalinity[DS_2013] &lt;- as.numeric(merged_df$emof$`salinity (psu)`[DS_2013])\nsalinity[DS_2019_2020] &lt;- as.numeric(merged_df$emof$`salinity_average (psu)`[DS_2019_2020])\n\n\nTemperature\ntemperature &lt;- rep(NA, nrow(merged_df$emof))\ntemperature[DS_2013] &lt;- as.numeric(merged_df$emof$`temperature (°C)`[DS_2013])\ntemperature[DS_2019_2020] &lt;- rowMeans(cbind(\n  as.numeric(merged_df$emof$`temperature_water_CTD_0-1m (°C)`[DS_2019_2020]),\n  as.numeric(merged_df$emof$`temperature_water_CTD_5m (°C)`[DS_2019_2020]),\n  as.numeric(merged_df$emof$`temperature_water_CTD_10m (°C)`[DS_2019_2020])\n))\n\n\n\n\nPrepare plotting symbols\nWe define dataset-specific plotting symbols:\npch &lt;- rep(NA, nrow(merged_df$counts))\npch[DS_2013] &lt;- 21\npch[DS_2019_2020] &lt;- 22\nWe also define a colour scale for seasonality:\ncolor_yday &lt;- colorRampPalette(\n  c(\"#2c7fb8\", \"#addd8e\", \"#edf8b1\", \"#fa9fb5\", \"#2c7fb8\")\n)(366)\n\n\n\nMap samples\nWe define a function for plotting samples by month:\nplot_map &lt;- function(dataset) {\n  par(mfrow = c(4, 4), mar = c(3, 3, 3, 3), xpd = TRUE)\n  for (i in 1:12) {\n    ix &lt;- intersect(dataset, which(month == i))\n    newmap &lt;- getMap(resolution = \"low\")\n    plot(\n      newmap,\n      xlim = c(11, 22),\n      ylim = c(62, 63),\n      asp = 1,\n      main = paste(\"Month\", i)\n    )\n    points(\n      lon[ix],\n      lat[ix],\n      col = \"black\",\n      bg = color_yday[yday][ix],\n      pch = pch[ix],\n      cex = 1.5 + as.numeric(salinity[ix]) / 20\n    )\n  }\n\n  plot(\n    1:365, rep(1, 365),\n    col = color_yday,\n    pch = \"|\",\n    cex = 3,\n    axes = FALSE,\n    ylim = c(0.9, 1.3)\n  )\n  axis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\n  text(182, 1.1, \"Day of year\", cex = 1.4)\n\n  plot(\n    c(0, 10, 20), rep(1, 3),\n    col = \"black\",\n    pch = 1,\n    cex = c(1.6, 2.4, 3.2),\n    xlim = c(-5, 25),\n    ylim = c(0.9, 1.3),\n    axes = FALSE\n  )\n  axis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\n  text(2, 1.1, \"Salinity\", cex = 1.4, adj = 0)\n}\nWe apply the function to each dataset:\nplot_map(DS_2013)\nplot_map(DS_2019_2020)\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-05-barplots.html",
    "href": "tutorial-05-barplots.html",
    "title": "5. Taxonomic composition and barplots",
    "section": "",
    "text": "Summarise taxonomic composition across salinity gradients and seasons using stacked barplots.\n\nDefine the barplot function\nThe function below aggregates monthly averages within salinity intervals (roughly corresponding to the basins Bothnian Bay, Bothnian Sea, Baltic Proper, Kattegat and Skagerrak) and plots stacked barplots for the top taxa.\nplot_barplots &lt;- function(rank, size_taxa = -1, main = \"\", top_x = 10) {\n  mycols &lt;- colorRampPalette(c(\n    \"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\",\n    \"#fb9a99\", \"#e31a1c\", \"#fdbf6f\", \"#ff7f00\",\n    \"#cab2d6\", \"#6a3d9a\", \"#ffff99\", \"#b15928\"\n  ))\n  par(mfrow = c(5, 1), mar = c(2, 3, 2, 14), xpd = TRUE)\n  salinity_intervals &lt;- rbind(c(0, 3), c(3, 6), c(6, 12), c(12, 20), c(20, 35))\n  ok &lt;- sort(rowMeans(cladecounts_df$norm[[rank]]), index.return = TRUE, decreasing = TRUE)$ix[1:top_x]\n  if (size_taxa == -1) { size_taxa &lt;- min(1.5, 8 / length(ok)) }\n  for (i in 1:5) {\n    ix &lt;- intersect(which(salinity &gt;= salinity_intervals[i,1]), which(salinity &lt; salinity_intervals[i,2]))\n    monthly_averages_matr &lt;- matrix(ncol = 12, nrow = nrow(cladecounts_df$norm[[rank]]))\n    for (j in 1:12) {\n      ix2 &lt;- intersect(ix, which(month == j))\n      if (length(ix2) &gt; 1) { monthly_averages_matr[, j] &lt;- rowMeans(cladecounts_df$norm[[rank]][, ix2]) }\n      if (length(ix2) == 1) { monthly_averages_matr[, j] &lt;- cladecounts_df$norm[[rank]][, ix2] }\n      if (length(ix2) == 0) { monthly_averages_matr[, j] &lt;- 0 }\n    }\n    barplot(monthly_averages_matr[ok, ], col = mycols(length(ok)),\n            main = paste(\"Salinity\", salinity_intervals[i,1], \"-\", salinity_intervals[i,2]))\n    legend(\"bottomleft\", bty = \"n\", pch = 19,\n           col = mycols(length(ok))[length(ok):1],\n           cex = size_taxa, inset = c(1, 0),\n           legend = rownames(cladecounts_df$norm[[rank]])[rev(ok)])\n  }\n}\n\n\nNote on taxonomic ranks\nUse numeric rank codes where: 1 = kingdom (or domain), 2 = phylum, 3 = class, 4 = order, 5 = family, 6 = genus, 7 = species.\nSome datasets may also contain OTU-level identifiers (e.g. BOLD-BINs, Unite SHs).\n\n\nExample: plot top classes\nPlot the 8 most abundant classes:\nplot_barplots(rank = 3, top_x = 8)\nThis will create five stacked barplot rows (one per salinity interval), each showing monthly averages for the selected taxa. You can also adjust the size of the taxa legend with the size_taxa parameter.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-05-barplots.html#goal",
    "href": "tutorial-05-barplots.html#goal",
    "title": "5. Taxonomic composition and barplots",
    "section": "Goal",
    "text": "Goal\nCreate monthly taxonomic barplots for salinity-defined basins and highlight dominant taxa."
  },
  {
    "objectID": "tutorial-05-barplots.html#steps",
    "href": "tutorial-05-barplots.html#steps",
    "title": "5. Taxonomic composition and barplots",
    "section": "Steps",
    "text": "Steps\n\nDefine the barplot function\nThe function below aggregates monthly averages within salinity intervals and plots stacked barplots for the top taxa.\nplot_barplots &lt;- function(rank, size_taxa = -1, main = \"\", top_x = 10) {\n  mycols &lt;- colorRampPalette(c(\n    \"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\",\n    \"#fb9a99\", \"#e31a1c\", \"#fdbf6f\", \"#ff7f00\",\n    \"#cab2d6\", \"#6a3d9a\", \"#ffff99\", \"#b15928\"\n  ))\n  par(mfrow = c(5, 1), mar = c(2, 3, 2, 14), xpd = TRUE)\n  salinity_intervals &lt;- rbind(c(0, 3), c(3, 6), c(6, 12), c(12, 20), c(20, 35))\n  ok &lt;- sort(rowMeans(cladecounts_df$norm[[rank]]), index.return = TRUE, decreasing = TRUE)$ix[1:top_x]\n  if (size_taxa == -1) { size_taxa &lt;- min(1.5, 8 / length(ok)) }\n  for (i in 1:5) {\n    ix &lt;- intersect(which(salinity &gt;= salinity_intervals[i,1]), which(salinity &lt; salinity_intervals[i,2]))\n    monthly_averages_matr &lt;- matrix(ncol = 12, nrow = nrow(cladecounts_df$norm[[rank]]))\n    for (j in 1:12) {\n      ix2 &lt;- intersect(ix, which(month == j))\n      if (length(ix2) &gt; 1) { monthly_averages_matr[, j] &lt;- rowMeans(cladecounts_df$norm[[rank]][, ix2]) }\n      if (length(ix2) == 1) { monthly_averages_matr[, j] &lt;- cladecounts_df$norm[[rank]][, ix2] }\n      if (length(ix2) == 0) { monthly_averages_matr[, j] &lt;- 0 }\n    }\n    barplot(monthly_averages_matr[ok, ], col = mycols(length(ok)),\n            main = paste(\"Salinity\", salinity_intervals[i,1], \"-\", salinity_intervals[i,2]))\n    legend(\"bottomleft\", bty = \"n\", pch = 19,\n           col = mycols(length(ok))[length(ok):1],\n           cex = size_taxa, inset = c(1, 0),\n           legend = rownames(cladecounts_df$norm[[rank]])[rev(ok)])\n  }\n}\n\n\nNote on taxonomic ranks\nUse numeric rank codes where: 1 = kingdom (or domain), 2 = phylum, 3 = class, 4 = order, 5 = family, 6 = genus, 7 = species.\nSome datasets may also contain OTU-level identifiers (e.g. BOLD-BINs, Unite SHs).\n\n\nExample: plot top classes\nPlot the 8 most abundant classes:\nplot_barplots(rank = 3, top_x = 8)\nThis will create five stacked barplot rows (one per salinity interval), each showing monthly averages for the selected taxa.\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-07-18S.html",
    "href": "tutorial-07-18S.html",
    "title": "7. Analyse 18S data (genus-level Bray–Curtis)",
    "section": "",
    "text": "Repeat the workflow using 18S data to demonstrate genus-level community analyses and PCoA.\n\nLoad 18S datasets\nSet the path to the downloaded 18S data and load it:\ndata_path &lt;- \"/srv/course-data/18S\"  \n# or, for data downloaded in Option B:\n# data_path &lt;- \"local/path/to/course-data/18S\"\n\nloaded &lt;- load_data(data_path)\n\n\nMerge datasets and convert to data frames\nmerged &lt;- merge_data(loaded)\nmerged_df &lt;- convert_to_df(merged, convert_counts = TRUE, max_cells = 1e9)\n\n\nAggregate taxa and convert to data frames\ncladecounts &lt;- sum_by_clade(merged$counts, merged$asvs)\ncladecounts_df &lt;- convert_to_df(cladecounts)\n\n\nPlot data on map\nWe can plot the datasets on the map using the plot_map function defined in 3:\n# Identify datasets\nDS_2013 &lt;- grep(\"KTH-2013-Baltic\", rownames(merged_df$events))\nDS_2019_2020 &lt;- grep(\"PRJEB55296\", rownames(merged_df$events))\n\n# Extract season/salinity again (adjust column names if needed)\nlat &lt;- merged_df$events$decimalLatitude\nlon &lt;- merged_df$events$decimalLongitude\nmonth &lt;- month(merged_df$events$eventDate)\nyday &lt;- yday(merged_df$events$eventDate)\n\nsalinity &lt;- rep(NA, nrow(merged_df$emof))\nsalinity[DS_2013] &lt;- as.numeric(merged_df$emof$`salinity (psu)`[DS_2013])\nsalinity[DS_2019_2020] &lt;- as.numeric(merged_df$emof$`salinity_average (psu)`[DS_2019_2020])\n\n# Set plotting symbols & colour scale\npch &lt;- rep(NA, nrow(merged_df$events))\npch[DS_2013] &lt;- 21\npch[DS_2019_2020] &lt;- 22\n\ncolor_yday &lt;- colorRampPalette(\n  c(\"#2c7fb8\", \"#addd8e\", \"#edf8b1\", \"#fa9fb5\", \"#2c7fb8\")\n)(366)\n\nplot_map(DS_2013)\nplot_map(DS_2019_2020)\n\n\nCheck primers\nunique(merged_df$events$pcr_primer_name_forward[DS_2013])\nunique(merged_df$events$pcr_primer_name_forward[DS_2019_2020])\n\nunique(merged_df$events$pcr_primer_name_reverse[DS_2013])\nunique(merged_df$events$pcr_primer_name_reverse[DS_2019_2020])\nDifferent primers were used in the two datasets!\n\n\n\nBray–Curtis distances on genus composition\nSince different primers were used, ASVs will likely not be shared and calculating Bray-Curtis based on ASVs will not give meaningful information (distances will be 1 between all cross-dataset sample pairs). Instead we calculate it based on genus-level counts.\nbray_dist &lt;- as.matrix(vegdist(t(cladecounts_df$norm$genus), method = \"bray\"))\n(Note that cladecounts$norm already provides normalised counts so we don’t need to do normalisation.)\n\n\nRun PCoA\npcoa &lt;- pcoa(bray_dist, correction = \"cailliez\")\n\n\nPlot PCoA (genus level)\nThis plot uses the same encoding as in the 16S PCoA section.\n\nlayout(matrix(c(1,1,1,1,2,2,2,2,3,4), 5, 2, byrow = TRUE))\npar(mar = c(5, 5, 1, 1), xpd = TRUE, cex.axis = 1)\n\nxlab &lt;- paste(\"PC1 \", round(pcoa$values$Eigenvalues[1]), \"%\", sep = \"\")\nylab &lt;- paste(\"PC2 \", round(pcoa$values$Eigenvalues[2]), \"%\", sep = \"\")\n\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2019–2020 (18S, genus)\")\npoints(pcoa$vectors[DS_2019_2020,1], pcoa$vectors[DS_2019_2020,2],\n       pch = pch[DS_2019_2020], col = \"black\",\n       bg = color_yday[yday[DS_2019_2020]],\n       cex = 1.0 + as.numeric(salinity[DS_2019_2020]) / 10)\n\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2013 (18S, genus)\")\npoints(pcoa$vectors[DS_2013,1], pcoa$vectors[DS_2013,2],\n       pch = pch[DS_2013], col = \"black\",\n       bg = color_yday[yday[DS_2013]],\n       cex = 1.0 + as.numeric(salinity[DS_2013]) / 10)\n\nplot(1:365, rep(1, 365), col = color_yday, pch = \"|\", cex = 3, axes = FALSE, ylim = c(0.9, 1.3))\naxis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\ntext(182, 1.2, \"Day of year\", cex = 2)\n\nplot(c(0, 10, 20), rep(1, 3),\n     col = \"black\", pch = 1, cex = c(1.6, 2.4, 3.2),\n     xlim = c(-5, 25), ylim = c(0.9, 1.3), axes = FALSE, ylab = \"\", xlab = \"\")\naxis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\ntext(10, 1.2, \"Salinity\", cex = 2)\n\n\n\nMake barplot\nWe can reuse the plot_barplot function defined in 5 to make taxonomic barplots. E.g.:\nplot_barplots(rank = 4, top_x = 8)\n← Previous · Overview"
  },
  {
    "objectID": "tutorial-07-18S.html#goal",
    "href": "tutorial-07-18S.html#goal",
    "title": "7. Analyse 18S data (genus-level Bray–Curtis)",
    "section": "Goal",
    "text": "Goal\nLoad 18S data, aggregate to genus level, compute Bray–Curtis distances, and visualise patterns with PCoA."
  },
  {
    "objectID": "tutorial-07-18S.html#steps",
    "href": "tutorial-07-18S.html#steps",
    "title": "7. Analyse 18S data (genus-level Bray–Curtis)",
    "section": "Steps",
    "text": "Steps\n\nLoad 18S datasets\nSet the path to the downloaded 18S data and load it:\ndata_path &lt;- \"~/bas_sbdi/sbdi/asvportal_data_downloads/20260202_18S_KTH_BalticSea\"\nloaded &lt;- load_data(data_path)\n\n\nMerge datasets\nmerged &lt;- merge_data(loaded)\n\n\nConvert to data frames (optional)\nmerged_df &lt;- convert_to_df(merged, convert_counts = TRUE, max_cells = 1e9)\n\n\nAggregate to genus\nWe aggregate read counts by taxonomic rank and extract the genus-level table:\ncladecounts &lt;- sum_by_clade(merged$counts, merged$asvs)\ncladecounts_df &lt;- convert_to_df(cladecounts)\n\ngenus_counts &lt;- cladecounts_df$raw$genus\nConvert to relative abundances per sample:\ngenus_norm &lt;- t(t(genus_counts) / colSums(genus_counts, na.rm = TRUE))\n\n\nBray–Curtis distances on genus composition\nbray_dist &lt;- as.matrix(vegdist(t(genus_norm), method = \"bray\"))\n\n\nRun PCoA\npcoa &lt;- pcoa(bray_dist, correction = \"cailliez\")\n\n\nPlot PCoA (genus level)\nThis plot uses the same encoding as in the 16S PCoA section.\n# Identify datasets (adjust patterns if needed)\nDS_2013 &lt;- grep(\"KTH-2013-Baltic\", merged_df$events$eventID)\nDS_2019_2020 &lt;- grep(\"PRJEB55296-\", merged_df$events$eventID)\n\n# Extract season/salinity again (adjust column names if needed)\nlat &lt;- merged_df$events$decimalLatitude\nlon &lt;- merged_df$events$decimalLongitude\nmonth &lt;- month(merged_df$events$eventDate)\nyday &lt;- yday(merged_df$events$eventDate)\n\nsalinity &lt;- rep(NA, nrow(merged_df$emof))\nsalinity[DS_2013] &lt;- as.numeric(merged_df$emof$`salinity (psu)`[DS_2013])\nsalinity[DS_2019_2020] &lt;- as.numeric(merged_df$emof$`salinity_average (psu)`[DS_2019_2020])\n\npch &lt;- rep(NA, nrow(merged_df$events))\npch[DS_2013] &lt;- 21\npch[DS_2019_2020] &lt;- 22\n\ncolor_yday &lt;- colorRampPalette(\n  c(\"#2c7fb8\", \"#addd8e\", \"#edf8b1\", \"#fa9fb5\", \"#2c7fb8\")\n)(366)\n\nlayout(matrix(c(1,1,1,1,2,2,2,2,3,4), 5, 2, byrow = TRUE))\npar(mar = c(5, 5, 1, 1), xpd = TRUE, cex.axis = 1)\n\nxlab &lt;- paste(\"PC1 \", round(pcoa$values$Eigenvalues[1]), \"%\", sep = \"\")\nylab &lt;- paste(\"PC2 \", round(pcoa$values$Eigenvalues[2]), \"%\", sep = \"\")\n\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2019–2020 (18S, genus)\")\npoints(pcoa$vectors[DS_2019_2020,1], pcoa$vectors[DS_2019_2020,2],\n       pch = pch[DS_2019_2020], col = \"black\",\n       bg = color_yday[yday[DS_2019_2020]],\n       cex = 1.0 + as.numeric(salinity[DS_2019_2020]) / 10)\n\nplot(pcoa$vectors[,1], pcoa$vectors[,2],\n     pch = pch, col = \"white\", bg = \"white\",\n     xlab = xlab, ylab = ylab, main = \"2013 (18S, genus)\")\npoints(pcoa$vectors[DS_2013,1], pcoa$vectors[DS_2013,2],\n       pch = pch[DS_2013], col = \"black\",\n       bg = color_yday[yday[DS_2013]],\n       cex = 1.0 + as.numeric(salinity[DS_2013]) / 10)\n\nplot(1:365, rep(1, 365), col = color_yday, pch = \"|\", cex = 3, axes = FALSE, ylim = c(0.9, 1.3))\naxis(1, at = c(1, 182, 365), labels = c(\"12\", \"182\", \"365\"), cex = 3)\ntext(182, 1.2, \"Day of year\", cex = 2)\n\nplot(c(0, 10, 20), rep(1, 3),\n     col = \"black\", pch = 1, cex = c(1.6, 2.4, 3.2),\n     xlim = c(-5, 25), ylim = c(0.9, 1.3), axes = FALSE, ylab = \"\", xlab = \"\")\naxis(1, at = c(0, 10, 20), labels = c(\"2\", \"18\", \"34\"), cex = 3)\ntext(10, 1.2, \"Salinity\", cex = 2)\n\n← Previous · Overview"
  },
  {
    "objectID": "tutorial-06-randomforest.html",
    "href": "tutorial-06-randomforest.html",
    "title": "6. Predict environmental parameters (Random Forest)",
    "section": "",
    "text": "Use Random Forest models to predict environmental parameters from community composition data.\n\nChoose target variable\nPick an environmental parameter to predict. For example, salinity:\ny &lt;- salinity\nRemove samples with missing values:\nok &lt;- which(!is.na(y))\ny &lt;- y[ok]\n\n\nCreate a feature matrix\nWe use relative abundances of ASVs as features (norm_counts created in 4).\nX &lt;- t(norm_counts) # rows = samples, cols = ASVs\nSubset to the samples with environmental parameter values:\nX &lt;- X[ok, , drop = FALSE]\nOptionally filter rare ASVs (present in &lt;10% of samples) to reduce dimensionality:\nkeep = which(colSums(X &gt; 0)/nrow(X) &gt;= 0.1)\nX = X[, keep, drop = FALSE]\n\n\nTrain/test split\nSplit the data into training and test sets to evaluate model performance on unseen samples.\nset.seed(1)\nn &lt;- nrow(X)\ntrain_ix &lt;- sample(seq_len(n), size = round(0.8 * n))\ntest_ix &lt;- setdiff(seq_len(n), train_ix)\n\nX_train &lt;- X[train_ix, , drop = FALSE]\ny_train &lt;- y[train_ix]\n\nX_test &lt;- X[test_ix, , drop = FALSE]\ny_test &lt;- y[test_ix]\n\n\nTrain Random Forest\nWe train a random forest model using the R package ranger.\nrf &lt;- ranger(\n  x = X_train,\n  y = y_train,\n  num.trees = 5000,\n  importance = \"permutation\"\n)\n\n\nPredict and evaluate\nAssess model performance by predicting the environmental parameter for the test data.\npred &lt;- predict(rf, data = X_test)$predictions\n\n# Coefficient of determination\nr2 = summary(lm(pred ~ y_test))$r.squared \nr2\n\n# Root Mean Square Error\nrmse &lt;- sqrt(mean((pred - y_test)^2, na.rm = TRUE)) \nrmse \n\n\nPlot observed vs predicted\nVisualize how well predicted values match observed values in the test set.\npar(mfrow = c(1,1), mar = c(5,5,2,2), xpd = FALSE)\nlims &lt;- range(c(y_test, pred))\nplot(y_test, pred,\n     xlab = \"Observed\",\n     ylab = \"Predicted\",\n     xlim = lims,\n     ylim = lims)\nabline(0, 1, lty = 2, col = \"grey50\")\n\n\nVariable importance (optional)\nIdentify which ASVs contribute most to the model’s predictions.\nimp &lt;- sort(rf$variable.importance, decreasing = TRUE)\nplot(imp)\nTo get the taxonomy of the 10 ASVs with highest importance for the model:\nmerged_df$asvs[names(imp[1:10]),7:10]\n\n← Previous · Overview · Next →"
  },
  {
    "objectID": "tutorial-06-randomforest.html#goal",
    "href": "tutorial-06-randomforest.html#goal",
    "title": "6. Predict environmental parameters (Random Forest)",
    "section": "Goal",
    "text": "Goal\nTrain and evaluate a Random Forest model that predicts an environmental variable (e.g. salinity) from ASV profiles."
  },
  {
    "objectID": "tutorial-06-randomforest.html#outline",
    "href": "tutorial-06-randomforest.html#outline",
    "title": "6. Predict environmental parameters (Random Forest)",
    "section": "Outline",
    "text": "Outline\nWe will:\n\nchoose a target variable (e.g. salinity or temperature)\nbuild a feature matrix from community data (ASVs or aggregated taxa)\nsplit into training and test sets\ntrain a Random Forest model\nevaluate prediction performance and inspect variable importance"
  },
  {
    "objectID": "tutorial-06-randomforest.html#steps",
    "href": "tutorial-06-randomforest.html#steps",
    "title": "6. Predict environmental parameters (Random Forest)",
    "section": "Steps",
    "text": "Steps\n\nChoose target variable\nPick an environmental parameter to predict. For example, salinity:\ny &lt;- salinity\nOptionally remove samples with missing values:\nok &lt;- which(!is.na(y))\ny &lt;- y[ok]\n\n\nCreate a feature matrix\nUse relative abundances as features. Here we start from merged_df$counts and normalise per sample:\nX &lt;- merged_df$counts\nX &lt;- t(t(X) / colSums(X, na.rm = TRUE))\nSubset to the samples in ok (if you removed missing targets above):\nX &lt;- t(X)[ok, , drop = FALSE]  # rows = samples, cols = ASVs\nOptionally filter rare ASVs to reduce dimensionality:\nkeep &lt;- which(colMeans(X, na.rm = TRUE) &gt; 0)\nX &lt;- X[, keep, drop = FALSE]\n\n\nTrain/test split\nset.seed(1)\nn &lt;- nrow(X)\ntrain_ix &lt;- sample(seq_len(n), size = round(0.8 * n))\ntest_ix &lt;- setdiff(seq_len(n), train_ix)\n\nX_train &lt;- X[train_ix, , drop = FALSE]\ny_train &lt;- y[train_ix]\n\nX_test &lt;- X[test_ix, , drop = FALSE]\ny_test &lt;- y[test_ix]\n\n\nTrain Random Forest\nChoose a Random Forest implementation available in your environment (e.g. randomForest, ranger, or randomForestSRC). Example using ranger:\n# install.packages(\"ranger\")\nlibrary(ranger)\n\nrf &lt;- ranger(\n  x = X_train,\n  y = y_train,\n  num.trees = 500,\n  importance = \"permutation\"\n)\n\n\nPredict and evaluate\npred &lt;- predict(rf, data = X_test)$predictions\n\ncor(pred, y_test, use = \"complete.obs\")\nrmse &lt;- sqrt(mean((pred - y_test)^2, na.rm = TRUE))\nrmse\n\n\nPlot observed vs predicted\nplot(y_test, pred,\n     xlab = \"Observed\",\n     ylab = \"Predicted\")\nabline(0, 1)\n\n\nVariable importance (optional)\nimp &lt;- sort(rf$variable.importance, decreasing = TRUE)\nhead(imp, 20)\n\n← Previous · Overview · Next →"
  }
]